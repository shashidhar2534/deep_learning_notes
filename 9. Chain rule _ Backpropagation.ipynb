{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9. Chain rule & Backpropagation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOTuX+NwwHo33/F7XP1jCKl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tLpHISbJWkSb"},"source":["#Deep Learning With Computer Vision And Advanced NLP (DL_CV_NLP)\n","\n","$$ Revision Notes $$\n","$$ A-Note-by-**Bappy Ahmed** $$"]},{"cell_type":"markdown","metadata":{"id":"muZHSwNhWlBc"},"source":["#Understanding of Chain Rule & Backpropagation:\n","\n","##Chain Rule:\n","In calculus, the chain rule is a formula to compute the derivative of a composite function. That is, if f and g are differentiable functions, then the chain rule expresses the derivative of their.\n","\n","### Lets get some derivation of chain rule:\n","\n","Suppose $f(x)= \\frac{1}{1+e^{-x}}$ is a function (it's a sigmoid function). Then what would be the $\\frac{df}{dx}=?$\n","\n","Let, $p=-x\\rightarrow p(x)$ \n","\n","and $q=1+e^{-x}= 1+e^p\\rightarrow q(p)$\n","\n","$\\therefore f(q)=\\frac{1}{q} $\n","\n","Now we can say,\n","\n","$f(q(p(x)))$\n","\n","\n","We know with respect to the derivative,\n","\n","$\\frac{df}{dq}= - \\frac{1}{q^2}$\n","\n","$\\frac{dq}{dp}= 0+e^p=e^p$\n","\n","$\\frac{dp}{dx}= -1$\n","\n","$\\therefore \\frac{df}{dx} = \\frac{df}{dq}*\\frac{dq}{dp}*\\frac{dp}{dx}$\n",">$=-\\frac{1}{q^2}*e^p*(-1)$\n","\n",">$=-\\frac{1}{(1+e^{-x)^2}}*e^{-x}*(-1)$\n","\n",">$=\\frac{e^{-x}}{(1+e^{-x)^2}}$\n","\n",">$=\\frac{e^{-x}}{(1+e^{-x)^2}} = \\frac{1+e^{-x}-1}{(1+e^{-x)^2}}$\n","\n",">$=\\frac{(1+e^{-x})}{(1+e^{-x)^2}} - \\frac{1}{(1+e^{-x)^2}}$\n","\n",">$=\\frac{1}{(1+e^{-x)}} - \\frac{1}{(1+e^{-x)^2}}$\n","\n",">$=\\frac{1}{(1+e^{-x)}} [1-  \\frac{1}{(1+e^{-x)}}]$\n","\n","$\\therefore \\frac{df(x)}{dx} = f(x) \\left \\{ 1-f(x) \\right \\}$\n","\n","This is the derivative of sigmoid function by the help of chain rule.\n"]},{"cell_type":"markdown","metadata":{"id":"FCz_0FkviKrX"},"source":["# Backpropagation:\n","\n","In deep learning, backpropagation is a widely used algorithm for training feedforward neural networks. Generalizations of backpropagation exist for other artificial neural networks, and for functions generally. These classes of algorithms are all referred to generically as \"backpropagation\". Backpropagation is nothing but it's chain rule.\n","\n","   <img src=\"https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Deep-learning-31-i2tutorials.png\" width=\"500\" \n","     height=\"300\">\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GXCBKb5s043Z"},"source":["###Lets get some derivation:\n","\n","Now let's take a simple neural network,\n","\n","\n","   <img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/18.png?raw=true\" width=\"600\" \n","     height=\"300\">\n","\n","\n","**Assumption**,\n"," - Each layer has 1 neuron\n"," - Bias = 0\n"," - Error function, $e = (y-\\hat{y})^2$\n"," - $y$ = actual value\n"," - $\\hat{y}$ = predicted value\n"," - $\\sigma$ is activation function (sigmoid)\n","\n","\n","weights update rule (Gradient Descent),\n","\n","$w= w - \\eta \\bigtriangledown e$\n","\n","Here, \n","\n","$\\bigtriangledown e = \\frac{\\partial e}{\\partial w}$\n","\n","$\\therefore w=w-\\eta \\frac{\\partial e}{\\partial w}$\n","\n","From the chain rule we found some dependency (use the figure above),\n","\n"," - $e= (y-\\hat{y})^2 = (y-a_2)^2\\rightarrow [f(a_2)]$  \n"," - $a_2 = \\sigma(z_2) \\rightarrow [f(z_2)]$\n"," - $z_2 = w_2.a_1 \\rightarrow [f(w_2)]$\n","\n","####Now lets find out $e$ w.r.t $w_2$\n","\n","####$\\frac{\\partial e}{\\partial w_2} = \\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2}*\\frac{\\partial z_2}{\\partial w_2}$\n","\n","$\\therefore \\frac{\\partial e}{\\partial w_2} = -2(y-a_2)*\\sigma(z_2)(1-\\sigma(z_2))*a_1$\n","\n","Now we can update the weight $w_2$ by,\n","$$w_2= w_2 - \\eta \\frac{\\partial e}{\\partial w_2} $$\n","\n","\n","#### Now lets find out $e$ w.r.t $w_1$\n","\n","Again from the chain rule we found some dependency (use the figure above),\n","\n"," - $e= (y-\\hat{y})^2 = (y-a_2)^2\\rightarrow [f(a_2)]$  \n"," - $a_2 = \\sigma(z_2) \\rightarrow [f(z_2)]$\n"," - $z_2 = w_2.a_1 \\rightarrow [f(a_1)]$\n"," - $a_1 = \\sigma(z_1) \\rightarrow [f(z_1)]$\n"," - $z_1 = w_1a_0 \\rightarrow [f(w_1)]$\n","\n","so, \n","\n","####$\\frac{\\partial e}{\\partial w_1} = \\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2}*\\frac{\\partial z_2}{\\partial a_1}*\\frac{\\partial a_1}{\\partial z_1}*\\frac{\\partial z_1}{\\partial w_1}$\n","\n","Note: \n","\n","  - We have already calculated $\\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2} =\\frac{\\partial e}{\\partial w_2} $\n","\n","\n","####$\\therefore \\frac{\\partial e}{\\partial w_1} = \\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2}*\\frac{\\partial z_2}{\\partial a_1}*\\frac{\\partial a_1}{\\partial z_1}*\\frac{\\partial z_1}{\\partial w_1}$\n","\n","\n","$\\therefore \\frac{\\partial e}{\\partial w_1} = \\frac{\\partial e}{\\partial w_2} * w_2*\\sigma(z_1)(1-\\sigma(z_1))*a_0$\n","\n","\n","Now we can update the weight $w_1$ by,\n","$$w_1= w_1 - \\eta \\frac{\\partial e}{\\partial w_1} $$\n","\n","- for updating bias we follow same strategy\n","\n","###This is the Backpropagation concept.\n","\n","##Note: Here we are calculating derivative of activation function also, So, it is crucial to chose a proper activation function.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"FFa25_4-PTbS"},"source":[""],"execution_count":null,"outputs":[]}]}