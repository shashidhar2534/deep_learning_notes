{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8. Gradient Descent.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMEPTXCRcsJftg8QqvXCbgj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d4UpjYMWYJk4"},"source":["#Deep Learning With Computer Vision And Advanced NLP (DL_CV_NLP)\n","\n","$$ Revision Notes $$\n","$$ A-Note-by-**Bappy Ahmed** $$"]},{"cell_type":"markdown","metadata":{"id":"SYn8ZqOXYLGH"},"source":["# Understanding of Gradient Descent:\n","\n","Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient of the function at the current point, because this is the direction of steepest descent.\n","\n","   <img src=\"https://cdn-images-1.medium.com/max/1600/0*fU8XFt-NCMZGAWND.\" width=\"500\" \n","     height=\"300\">"]},{"cell_type":"markdown","metadata":{"id":"GkoH74CtYw5l"},"source":["In neural network we have to minimize the cost or error.So that to reduce this error we use different types of optimizer, Here we will discuss one of them called Gradient Descent. It's an optimization algorithm.\n","\n","- In Neural netwoks we give some inputs and initialize some random weights & biases. Also define some activation functions, cost functions(it calculates the error) as well. Based on that by the help of optimizer a neural network automatically updates the weights & biases.\n","\n"," -    <img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/15.png?raw=true\" width=\"500\" \n","     height=\"300\">\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LdXGhpchawh-"},"source":["### Now lets get some derivation\n","\n","Let's take a simple cost function,\n","- $f = (y-\\hat{y})^2$\n","\n","- >$= (y-\\phi(w.x+b))^2$  [Here, $\\phi$ is an activation function]\n","\n","- Weight update rule:\n","$$w= w+\\Delta w$$\n","\n","$$b = b + \\Delta b$$\n","\n","Here,$$\\Delta w = -\\eta \\bigtriangledown f\\rightarrow (cost)$$ \n"," \n","\n","$$= -\\eta \\frac{\\partial f}{\\partial w}$$\n","\n","So the Gradient Descent formula is,\n","\n","$$w= w - \\eta\\frac{\\partial f}{\\partial w}$$ \n","\n","Here, $\\eta$ is a learning rate.\n"]},{"cell_type":"markdown","metadata":{"id":"SfO_46bYgTiI"},"source":["cost function,\n","- $f = (y-\\hat{y})^2$\n","\n","$f = (y-\\phi(w.x+b))^2$  [Here, $\\phi$ is an activation function]\n","\n","Lets take an assumption, $y =k$   as it is an actual value.\n","\n","\n","- Assumption, \n","$$b = 0$$\n","and$$\\phi(w.x+b)= \\phi(w.x)$$\n","\n","$$if, \\phi = ReLu \\rightarrow \\left\\{\\begin{matrix}\n","w.x & w.x\\geq 0 \\\\  \n","0 & w.x < 0\n","\\end{matrix}\\right.$$\n","\n","$$if, \\phi = sigmoid(\\sigma)= \\sigma(w.x)$$ \n","    \n","- The entire value will be between 0 to 1\n","\n","$\\phi(w.x)$ returns some value which depends on w.\n","            "]},{"cell_type":"markdown","metadata":{"id":"k3ryWPVWmdr-"},"source":["Again assumption,\n","$$\\phi(w.x) = W $$\n","\n","- $f= (y- \\phi(w.x+b))^2$\n","- After assumption,\n","  - $f= (y-W)$\n","  - $f(W) = (1- W)^2$  [Let $y=1$]\n","\n","\n","\n","- ### Now see Example:\n","- First of all initilize weights randomly\n","- Calculate the error with respect to all the weights & plot them like that.\n","<img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/16.png?raw=true\" width=\"500\" \n","     height=\"300\">\n","\n","\n","  - Suppose $W=3$\n","  - so, $f(3)= (1-3)^2= 4$\n","\n","#### Calculate gradient of $f$,\n","$$\\bigtriangledown f(W) = \\frac{\\partial f}{\\partial w}*i$$\n","\n","Take derivative,\n","$$\\frac{\\partial f}{\\partial w} = \\frac{\\partial (1-w)^2}{\\partial w}$$\n","\n","$$= 1+w^2-2w$$\n","\n","Now take second order derivative,\n","$$\\frac{\\partial^2 f}{\\partial w^2} = \\frac{\\partial^2 (1+w^2-2w)}{\\partial w^2}$$\n","\n","$$= 0 +2w-2$$\n","\n","$$\\therefore \\frac{\\partial f}{\\partial w} = 2(w-1)$$\n","\n","Now,  \n","$$\\bigtriangledown f(w=3) = 2(3-1)i$$\n","\n","$$= 4i$$ \n","Now we got the slope of the function at $w=3$.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mioD_6eKuMB4"},"source":["```By default when you use gradient descent, it gives the ascent of of gradient. To solve that we use (-) at the first of the equation```, $-\\bigtriangledown f$.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AUaSMediu9Sq"},"source":["## Updating weights:\n","\n","$\\bigtriangledown f(3)= 4i$\n","\n","- If we don't use (-) then it would be gradient ascent.\n","    - $w= w+\\bigtriangledown f(3)$\n","       \n","        $=3+4$\n","\n","        $=7$\n","\n","- If we don't use learning rate $\\eta$ then the target will get overshooting.\n","\n","     - $w= w-\\bigtriangledown f(3)$\n","       \n","        $=3-4$\n","\n","        $=-1$\n","\n","\n","So the proper equation should be,\n","\n","$$w= w-\\eta\\bigtriangledown f(3)$$\n","       $$ =w- 0.1*4$$ \n","        $$=3-0.4$$\n","        $$= 2.6$$ \n","\n","Here just assume $\\eta= 0.1$\n","\n","       \n","        "]},{"cell_type":"code","metadata":{"id":"YaYDfsPn5nKi"},"source":[""],"execution_count":null,"outputs":[]}]}