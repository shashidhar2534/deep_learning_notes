{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10. Problem faces NN & weight initialization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO+oIrVJSuE2jh2cmQ45RVQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f2yMGj75RZVq"},"source":["#Deep Learning With Computer Vision And Advanced NLP (DL_CV_NLP)\n","\n","$$ Revision Notes $$\n","$$ A-Note-by-**Bappy Ahmed** $$"]},{"cell_type":"markdown","metadata":{"id":"t1ccNy4pRd8P"},"source":["# Understanding problem faces in training Neural Networks:\n","\n"," ### 1. Vanishing & Exploding Gradient.\n","   - **Vanishing Gradient**: Gradient becomes very small.\n","   - **Exploding Gradient**: Gradient becomes very huge.\n","  \n","  That's also called unstable gradient problem that can never solved.\n","\n"," ### 2. It requires lot of data to train.\n","      - It can be solved by the help of Transfer Learning or Data augmentation techniques.\n","\n"," ### 3. Increasing the size of NN means increasing the no. of hidden layers. So, it may cause slow training.\n","    - Solution can be go for a better optimizer or a better activation function as well.\n","\n"," ### 4. Risk of overfitting (Milions of parameters).This also happens for not enough data or for noisy data.\n","    - Solution can be Regularization or dropout.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fObEz3MZVStt"},"source":["# Vanishing & Exploding Gradient:\n","\n","\n","So here, in the situation where the value of the weights is larger than 1, that problem is called exploding gradient because it hampers the gradient descent algorithm. When the weights are less than 1 then it is called vanishing gradient because the value of the gradient becomes considerably small with time.\n","\n","Now lets see about Vanishing & Exploding Gradient. Lets take a simple network.\n","\n","   <img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/18.png?raw=true\" width=\"600\" \n","     height=\"300\">\n","\n","\n","**Assumption**,\n"," - Each layer has 1 neuron\n"," - Bias = 0\n"," - Error function, $e = (y-\\hat{y})^2$\n"," - $y$ = actual value\n"," - $\\hat{y}$ = predicted value\n"," - $\\sigma$ is activation function (sigmoid)\n","\n","\n","weights update rule (Gradient Descent),\n","\n","$w= w - \\eta \\bigtriangledown e$\n","\n","Here, \n","\n","$\\bigtriangledown e = \\frac{\\partial e}{\\partial w}$\n","\n","$\\therefore w=w-\\eta \\frac{\\partial e}{\\partial w}$\n","\n","From the chain rule we found some dependency (use the figure above),\n","\n"," - $e= (y-\\hat{y})^2 = (y-a_2)^2\\rightarrow [f(a_2)]$  \n"," - $a_2 = \\sigma(z_2) \\rightarrow [f(z_2)]$\n"," - $z_2 = w_2.a_1 \\rightarrow [f(w_2)]$\n","\n","####Now lets find out $e$ w.r.t $w_2$\n","\n","####$\\frac{\\partial e}{\\partial w_2} = \\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2}*\\frac{\\partial z_2}{\\partial w_2} \\rightarrow (1)$\n","\n","$\\therefore \\frac{\\partial e}{\\partial w_2} = -2(y-a_2)*\\sigma(z_2)(1-\\sigma(z_2))*a_1$\n","\n","Now we can update the weight $w_2$ by,\n","$$w_2= w_2 - \\eta \\frac{\\partial e}{\\partial w_2} $$\n","\n","\n","#### Now lets find out $e$ w.r.t $w_1$\n","\n","Again from the chain rule we found some dependency (use the figure above),\n","\n"," - $e= (y-\\hat{y})^2 = (y-a_2)^2\\rightarrow [f(a_2)]$  \n"," - $a_2 = \\sigma(z_2) \\rightarrow [f(z_2)]$\n"," - $z_2 = w_2.a_1 \\rightarrow [f(a_1)]$\n"," - $a_1 = \\sigma(z_1) \\rightarrow [f(z_1)]$\n"," - $z_1 = w_1a_0 \\rightarrow [f(w_1)]$\n","\n","so, \n","\n","####$\\frac{\\partial e}{\\partial w_1} = \\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2}*\\frac{\\partial z_2}{\\partial a_1}*\\frac{\\partial a_1}{\\partial z_1}*\\frac{\\partial z_1}{\\partial w_1}$\n","\n","Note: \n","\n","  - We have already calculated $\\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2} =\\frac{\\partial e}{\\partial w_2} $\n","\n","\n","####$\\therefore \\frac{\\partial e}{\\partial w_1} = \\frac{\\partial e}{\\partial a_2}* \\frac{\\partial a_2}{\\partial z_2}*\\frac{\\partial z_2}{\\partial a_1}*\\frac{\\partial a_1}{\\partial z_1}*\\frac{\\partial z_1}{\\partial w_1}\\rightarrow (2)$\n","\n","\n","$\\therefore \\frac{\\partial e}{\\partial w_1} = \\frac{\\partial e}{\\partial w_2} * w_2*\\sigma(z_1)(1-\\sigma(z_1))*a_0$\n","\n","\n","Now we can update the weight $w_1$ by,\n","$$w_1= w_1 - \\eta \\frac{\\partial e}{\\partial w_1} $$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iMPPzNHOWs6-"},"source":["### Major Observations:\n","\n"," 1. All components of equation $(1)$ and $(2)$ are kind of ratio.\n","\n"," 2. If some how this ratio output between 0 and 1. Then weight update will be no change or very less changes. So , it would be vanishing gradient. If you have more number of ratio or products  then it will be more less value and the learning will be very slow (minor update).Lower layer is hard to train. This is called vanishing gradient.\n","\n","3. If ratio or product term is > 1 then it calls Exploding Gradient. This problem is faced by RNN frequently. In this problem your solution will be diverge.\n","\n","## Note: \n","Vanishing & Exploding gradient depend upon your choice of activation function and weight initialization technique as well. It was observed in 2010 by Xavier Glorot and Yoshua Bengio in their paper ```Understanding the difficulty of training deep feedforward neural networks``` [Paper link](http://proceedings.mlr.press/v9/glorot10a)\n","\n","Vanishing & Exploding gradient >> Each layer learn at different speed.\n","\n","\n","    "]},{"cell_type":"markdown","metadata":{"id":"Z9JcWMnXpSx1"},"source":["### Lets have an observation w.r.t sigmoid function,\n","\n","   <img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/17.png?raw=true\" width=\"600\" \n","     height=\"300\">\n","\n","\n","\n","## Case 1.\n","\n","Assumption,\n","\n","- $bias = 0 $\n","- Random initialization weight $w= 10$\n","- and $x=1$\n","\n","so,\n","\n","$z= w.x= 10$\n","\n","> $\\sigma(z=10) = \\frac{1}{1+e^{-10}}$\n",">>>>$= \\frac{1}{1+0}$\n","\n",">>>>$\\approx 1$\n","\n","Now find out the derivative at $z=10$ of sigmoid function,\n","\n","we know derivation of sigmoid function is,\n","\n","$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z)(1-\\sigma(z))$\n","\n","$=\\frac{\\partial \\sigma(10)}{\\partial z} = 1(1-1)= 0$\n","\n","As you can see the gradient is zero, So, the weight won't get updated.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yQcYZ82tudnF"},"source":["## Case 2.\n","\n","Assumption,\n","\n","- $bias = 0 $\n","- Random initialization weight $w= -10$\n","- and $x=1$\n","\n","so,\n","\n","$z= w.x= -10$\n","\n","> $\\sigma(z=-10) = \\frac{1}{1+e^{10}}$\n","\n",">>>>$\\approx 0$\n","\n","Now find out the derivative at $z=-10$ of sigmoid function,\n","\n","we know derivation of sigmoid function is,\n","\n","$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z)(1-\\sigma(z))$\n","\n","$=\\frac{\\partial \\sigma(-10)}{\\partial z} = 0(1-0)= 0$\n","\n","As you can see the gradient is zero, So, the weight won't get updated.\n"]},{"cell_type":"markdown","metadata":{"id":"BDNPjdFxvGoW"},"source":["## Conclution:\n","Vanishing & Exploding gradient depend upon your choice of activation function and weight initialization technique as well. we have seen the proved above. So, instead of using random initialization of weight we have have to use some other technique such that our model don't get Vanishing & Exploding gradient problem. On the figure we can see we should initilize the weight between slope region not in saturation region. We can also do that expriment with other activation function as well.\n"]},{"cell_type":"markdown","metadata":{"id":"IRYRHNEMGxLS"},"source":["### connection weight must be initialized such a way,\n","\n","$$fan_{avg} = \\frac{fan_{in}+fan_{out}}{2}$$\n","\n","\n","\n","   <img src=\"https://github.com/entbappy/Branching-tutorial/blob/master/20.png?raw=true\" width=\"500\" \n","     height=\"300\">\n","\n","\n","Refference: [Paper link](http://proceedings.mlr.press/v9/glorot10a)\n"]},{"cell_type":"markdown","metadata":{"id":"qhYL1mBJFKyc"},"source":["## Weights initialization techniques are,\n"," - glorot or Xavier Glorot $\\rightarrow$ None, tanh, sigmoid, logistic, softmax\n"," - he or Kaiming he $\\rightarrow$ ReLu & its variants\n"," - lecun or Yann Lecun $\\rightarrow$ SELU"]},{"cell_type":"code","metadata":{"id":"-WAAR45NRBCg"},"source":[""],"execution_count":null,"outputs":[]}]}